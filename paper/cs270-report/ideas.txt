Achlioptas and Iliopoulos 2014: Random walks that find perfect objects (and Papadimitriou 1991)
  Setup: We generically have a large set \Omega (think: the set of possible assignments) and a collection of subsets F called flaws (think: each subset f \in F is the set of assignments that violate a constraint, e.g. the set of 2^{n-k} assignments in which all the variables in a clause are wrong and the other variables are free), and we want to find an element of \Omega that isn't in the union of all the flaws.  We say such elements are ``perfect''.  The case where \Omega is a product is an important special case, but it is unnecessary.
  Consider the class of algorithms that iteratively move around in \Omega.  We can encode a broad subclass of these via a directed multigraph on \Omega called the transformation graph.  For each flaw f of an element \sigma \in \Omega, our algorithm will have a set of possible ``actions'' it can take to ``address'' that flaw; an action moves us to a (potentially) different state \sigma' \in \Omega.  So we can encode an action as an edge from \sigma to \sigma' labeled f.  If we require that each node has, for each of its flaws, at least one potential action that can move us to a strictly different element \sigma', then a node is a sink in this graph iff it is perfect.  This is useful because a random walk with any positive weights along the edges will eventually find a perfect node if the graph is, for example, weakly connected.  (We an add self-edges to sink nodes and then we can think of this as a legimitate Markov chain.)
    Note that that statement is true even if there are many edges to imperfect nodes, or many self-edges.  However, the convergence could be slow in general, and putting additional requirements on the graph will help us prove things.
    Note that traditional results on mixing times are not useful, since the whole point is to see how long it takes to \emph{leave} non-ergodic parts of the graph, given that we start there.
    It would be nice to have terminology for the case when \Omega has a product representation respected by F.  We could call them CSPs, and I think this is Achlioptas' and Iliopoulos' definition.
    What if perfect nodes are only weak sinks in some sense -- maybe they are members of a strongly connected component with high stationary probability, for example?  Perhaps this could be useful for distributed algorithms where we might move away from a perfect node.
  Rather than proving something about mixing times, Moser and Tardos use a novel entropy-based argument.  This argument requires that we can reconstruct the entire history of a walk given its endpoint and the history of addressed clauses.  (That takes n + (running time)*log(m) bits in the k-SAT example.)  This motivates the requirement of ``atomicity''.
    Papadimitriou uses a different proof for his algorithm and k-SAT: Consider the Hamming distance from our current assignment to a satisfying one.  P(distance decreases) >= 1/2, and the maximum distance is n, so a gambler's ruin argument says we finish in O(n^2) time whp.  How is this argument related?  Can the Hamming distance idea be generalized to arbitrary graphs, where there may in general be no embedding of \Omega in {0,1}^n?
    Is the entropy argument really the way to go?  It seems more fragile than mixing-time-based arguments.  For example, what if we made the transformation graph complete by adding a very small weight to every edge?  Maybe we could then prove that perfect nodes have high stationary probability and the mixing time of the (now ergodic) graph is small.  Presumably someone has thought about this, though.
    Can the entropy argument be made less fragile?  See below on atomicity.
  Atomicity is: For every (\omega, f) pair there is at most one edge labeled f incoming at \omega.  In CSPs where flaws correspond to constraints, it is sufficient that each clause forbids exactly one assignment to its variables, and that the algorithm modifies only those variables when it addresses that flaw.
  In the graph coloring homework problem, it was important to encode the constraints as "not two neighbors equal color c" for each possible color c, rather than "not two neighbors are equal".  Here this is explained by the requirement of atomicity.  If we have C colors, then there would be C predecessors for each (node, flaw) pair.  However, as Achlioptas and Iliopoulos point out, this is merely a syntactic requirement that does not change the actual algorithm.  It only changes our analysis.  In that case, it made the size of F larger, so recording the flaws addressed at each step would take log C more bits.  But we do not need to record the flaws in the actual algorithm, only imagine doing so when we want to use an entropy argument in a proof.
    Are there reasonable general procedures for converting a non-atomic graph for a CSP to an atomic one?
    Actually we can refine the witness from the size of F to the dependency graph, which we will discuss later.  That is, if we know we ended up at node \omega, and U(\omega) is the set of flaws of \omega, then [\cup_{f \in U(\omega)} CanCause(f)] is the set of flaws that a predecessor of \omega could have had.  (For example, in a CSP, if two constraints have disjoint variables, and both are satisfied in \omega, then no predecessor of \omega could unsatisfy both.  FIXME: I don't quite understand this argument yet.)  By atomicity each flaw that could have been addressed in a predecessor corresponds to one predecessor of \omega, so if we know the flaw that was addressed, we know the predecessor.  So we need only (log of this size of this set) to encode each f in the history.
    What would we do here without atomicity?
  Why is the Moser-Tardos algorithm different from, e.g., Papadimitriou's algorithm?  Intuitively, we want to maximize the entropy in a random walk under the atomicity constraint.  Selecting a single variable at a time reduces the number of edges substantially.  Selecting all the variables, and selecting an assignment uniformly at random, increases the entropy while maintaining atomicity.
    Does it maximize entropy (in some sense, uniformly across random starts) under that constraint, for CSPs?  Could be interesting to prove that.
    Do Moser and Tardos motivate their algorithm that way?
    Given an arbitrary (\Omega, F), can we find the algorithm that maximizes entropy among all algorithms with atomic graphs?
    Is it possible and useful to violate atomicity a little in order to increase entropy a lot?  This would probably generate a ``noisy compression'' algorithm, and maybe we can still make an information-theoretic argument about that.  If we don't know for sure which path was taken through the graph, can we at least compute a posterior on the path?
  The generalization of the undirected dependency graph is the causality digraph.  There is an edge from f to g if f ``potentially causes'' g.  We say that f potentially causes g if f causes g for any pair of nodes (\sigma, \tau), and we say that f causes g for (\sigma, \tau) if there is an edge (\sigma, \tau, f) and either f = g or g is not a flaw in \sigma.  In an algorithm for a CSP, this is saying that f potentially causes g if an action that addresses f can change g from satisfied to unsatisfied for some pair of nodes.  If the algorithm is atomic, this implies that f and g share a variable, and since this relation is reflexive, the causality digraph for an atomic algorithm on a CSP must be undirected.
  We want the causality graph to be sparse because it allows us to efficiently encode the label of an edge in the transformation graph.  At the same time, we want the transformation graph to be dense because it increases the amount of randomness used by the algorithm while traversing it.  (And we probably want to use uniform random transitions, since that maximizes entropy.)
  Can the Achlioptas-Iliopoulos version of the algorithm be parallelized?
    Not clear, since it requires that the addressed clause is the max under an order on the clauses that is fixed ahead of time.  Before we do an action, we cannot be sure whether a clause with higher ID will be introduced by the action.
    Perhaps rereading Moser-Tardos will help with this, since they originally phrase the algorithm that way.
  [Read, but did not understand, proofs.]

Freer et al 2010: When are probabilistic programs computationally tractable?
  They view the MT algorithm as an MCMC method with ``an unusual termination condition''.  More explicitly, the point is that the walk graph is not ergodic, and (in most versions of the algorithm) only the perfect nodes are ergodic.  So spectral methods don't really apply here.  Also, they don't mention whether the MT algorithm is a special case of any particular MCMC inference algorithm; the set of selected nodes for resampling at each iteration seems rather unusual.  This is my question: Is there an analogous algorithm to MT in MCMC inference?  Are there LLL algorithms that would work well for certain MCMC inference problems, or vice versa?
  They mostly just cite Haeupler et al 2010.

Haeupler et al 2010: New Constructive Aspects of the Lovasz Local Lemma
  Contributions:
    Introduces the conditional-LLL distribution: The distribution of variable settings (or more generally of objects in the Achlioptas setup) conditional on no events happening.  Shows that the MT algorithm approximately (though not exactly) samples from this distribution.
    Under stronger-than-LLL conditions, shows that the MT algorithm takes time linear in n (= \log \Omega in the Achlioptas setup?), with almost no dependence on m.  Does the Achlioptas paper show this?
    Shows that we can even avoid finding violated events when this is hard, by sampling a polynomially-sized core of events to check.  Is this analogous to the case where likelihoods are hard to compute in importance sampling?
    Shows that we can find solutions that satisfy many events even when satisfying them all is impossible (or at least the LLL conditions are not met).  More precisely, we can do better than random sampling for NP-hard problems like MAX-SAT.
  (These notes are from the lecture notes here: http://www.cs.princeton.edu/~zdvir/apx11slides/Srinivasan_scribe.pdf)

Original idea(?): Low exponential running time for MT-like algorithms on k-SAT
  Problem with this is that the clause selection is arbitrary, so we could have selected a clause where all but 1 variable is in the right configuration, and in that case on average we move [(k-1)/2 - 1] away from 0.

Original idea(?): Can we make the same entropy argument if the transformation graph is not atomic?
  We needed atomicity so that there would be a 1-1 correspondence between 